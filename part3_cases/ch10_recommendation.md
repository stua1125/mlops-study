# CHAPTER 10. 마케팅 추천 엔진

이 장은 **추천 시스템**을 사례로 MLOps 전 과정을 연결합니다. 대규모 데이터 파이프라인, 온라인/배치 서빙, 실험 관리, 모니터링과 피드백까지 실무 관점에서 다룹니다.

---

## 10.1 추천 엔진의 반란
- 전통적 룰 기반/협업 필터링에서 **딥러닝·하이브리드**로 진화
- 비즈니스 목표(전환율, 체류시간, LTV)에 맞춘 **목표 지표(offline/online)** 정렬
- 콜드스타트, 스파스 데이터, 다중 목표(노출/클릭/매출) 트레이드오프

## 10.2 데이터 준비
- **원천 데이터**: 유저 행동 로그(뷰/클릭/장바구니/구매), 아이템 메타, 컨텍스트(시간·디바이스)
- **피쳐 설계**: ID 임베딩, 카테고리/텍스트/이미지 특성, 세션/순차 정보, 최신성(Recency)
- **라벨링**: 클릭/구매 등 이진·다중 라벨, **네거티브 샘플링**, 시간 누수 방지(시계열 스플릿)

## 10.3 실험 설계 및 관리
- **오프라인 평가**: Hit@K, NDCG@K, MAP, AUC — cold/warm 분할로 세트 구성
- **온라인 실험(A/B)**: 유저/세션 단위 랜덤화, 노출 편향·동일성(Fairness) 고려
- **실험 추적**: 파라미터·지표·아티팩트(피쳐/임베딩/모델) 기록(Mlflow 등)

## 10.4 모델 학습 및 배포
- 베이스라인: **MF/ALS/LightFM**, 콘텐츠 기반; 고급: DLRM/Two-Tower/Seq2Seq/Transformer
- **배치 추천**(주기적 리랭킹) + **실시간 후보생성/재순위화** 혼합
- 피쳐 스토어/임베딩 스토어 일관성 관리(학습-서빙 스큐 방지)

## 10.5 파이프라인 구조와 배포 전략
- 데이터 수집 → 피쳐 생성 → **후보군 생성(ANN/FAISS/ScaNN)** → 재순위화 → 노출/로그 수집
- 서빙: 마이크로서비스/Feature Store/Vector DB(ANN) + 캐시 계층
- 배포: **블루–그린 / 카나리 / Shadow**; 롤백과 버전 고정(아티팩트/피쳐 스키마)

## 10.6 모니터링과 피드백
- **모델/피쳐 드리프트**, 클릭률/전환율/수익 지표의 온라인 모니터링
- **탐색–활용(Exploration–Exploitation)** 균형: Thompson/UCB, ε-greedy 등
- 사용자 피드백·리뷰/신호로 **액티브 러닝** 및 주기적 재학습 트리거

## 10.7 마치며
- 추천 시스템은 **데이터 엔지니어링·서빙·실험문화**가 맞물려 성과가 난다
- 오프라인/온라인 지표 정렬, 실험 추적, 안전한 배포와 모니터링이 핵심
